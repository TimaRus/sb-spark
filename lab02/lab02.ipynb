{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.util.Try\n",
    "import java.net.{ URL => url }\n",
    "import java.net.URLDecoder.{ decode => dec }\n",
    "import org.apache.spark.sql.Column\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.Dataset\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.execution.command.ExplainCommand\n",
    "import org.apache.spark.sql.functions.broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val decode_url = udf { (url: String) => Try(new url(dec(url)).getHost).toOption }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val autousers = spark.read.json(\"/labs/laba02/autousers.json\").select(explode('autousers).alias(\"UID2\"))\n",
    "\n",
    "val csvOptions = Map(\"delimiter\"->\"\\t\", \"inferSchema\" -> \"true\")\n",
    "\n",
    "val logsRaw = spark.read.options(csvOptions).csv(\"/labs/laba02/logs\")\n",
    "//val logsRaw = spark.read.options(csvOptions).csv(\"/labs/laba02/logs/part-00000\")\n",
    "\n",
    "val logs =\n",
    "    logsRaw\n",
    "      .filter(col(\"_c2\").isNotNull)\n",
    "      .filter(col(\"_c2\").contains(\"http\"))\n",
    "      .withColumn(\"URLClean\", decode_url(col(\"_c2\")))\n",
    "      .withColumn(\"URLClean\", regexp_replace(col(\"URLClean\"), \"^www\\\\.\", \"\"))\n",
    "      .select(col(\"URLClean\"), col(\"_c0\").alias(\"UID\"))\n",
    "\n",
    "//logs.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val left = logs.select('URLClean, 'UID)\n",
    "val right = autousers.select('UID2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.when\n",
    "\n",
    "val URLActual = \n",
    "    left\n",
    "        .join(right, left(\"UID\") === right(\"UID2\"), \"left\")\n",
    "        .select('URLClean.as(\"URL\"), 'UID2.as(\"Auto_flag\"))\n",
    "        .withColumn(\"Auto_flag\", when($\"Auto_flag\".isNull, 0).otherwise(1))//.take(50)\n",
    "\n",
    "//URLActual.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "val windowCountry = Window.partitionBy(\"URL\")\n",
    "\n",
    "val result = URLActual.localCheckpoint\n",
    "                //.withColumn('URL).as(\"domain\")\n",
    "                .withColumn(\"cnt_visits\", count(\"*\").over(windowCountry))\n",
    "                .withColumn(\"cnt_flags\", sum(\"Auto_flag\").over(windowCountry))\n",
    "                .withColumn(\"cnt_flags_all\", sum(\"Auto_flag\")over(Window.partitionBy()))\n",
    "                .withColumn(\"cnt_visits_all\", count(\"*\").over(Window.partitionBy()))\n",
    ".withColumn(\"relevance\", (('cnt_flags/'cnt_visits_all) * ('cnt_flags/'cnt_visits_all)) / (('cnt_visits/'cnt_visits_all) * ('cnt_flags_all/'cnt_visits_all)))\n",
    ".withColumn(\"relevance\", round(col(\"relevance\"), 15))\n",
    "//result.select('URL, 'Auto_flag, 'cnt_visits, 'cnt_flags, 'cnt_flags_all, 'cnt_visits_all, 'relevance)\n",
    ".select('URL.as(\"domain\"), 'relevance)\n",
    "\n",
    "//result.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val res = result.groupBy(\"domain\").agg(max(\"relevance\").as(\"relevance\"))\n",
    ".orderBy('relevance.desc,'domain)\n",
    "\n",
    "//.show(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val itog = res.limit(200).collect()\n",
    "\n",
    "import scala.util.Try\n",
    "import java.io._\n",
    "\n",
    "def printToFile(f: java.io.File)(op: java.io.PrintWriter => Unit) {\n",
    "  val p = new java.io.PrintWriter(f)\n",
    "  try { op(p) } finally { p.close() }\n",
    "}\n",
    "\n",
    "val rowsToFile = itog.map(r => s\"${r(0)}\\t${\"%.20f\".format(r(1))}\")\n",
    "\n",
    "printToFile(new File(\"laba02_domains.txt\")) {\n",
    "    p => rowsToFile.foreach(p.println)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
